#!/usr/bin/env bds

#-------------------------------------------------------------------------------
# Aggregate results after running analysis (on individual samples)
#
# FIXME: This class should be a 'AggregationBcbio' which inherits from the Aggregation class
#
#-------------------------------------------------------------------------------
class Aggregation extends Pipeline {
	MultiQc multiQc
	Seq2c seq2c
	SequencingData sequencingData
	SolveBio solveBio
	string ngbJsonFilePath = ''

	void Aggregation(SequencingData sd) {
		println( "hi/2", "$samplePath/programs.txt" )
		sequencingData = sd
		solveBio = new SolveBio(sd, solveBioUploadImport, solveBioVault)
		multiQc = new MultiQc(sequencingData)
	}

	void Aggregation(SequencingData sd, Seq2c seq2c) {
		this.Aggregation(sd)
		this.seq2c = seq2c
	}

	void combineConfigurationFiles() {
		# Per-sample programs.txt and data_versions.csv are identical and one should be uploaded to common latest_bcbio folder
		# Per-sample project-summary.yaml are not identical and should be combined and then uploaded to the common latest_bcbio folder
		string[] projectSummaryFiles
		projPath := sequencingData.bcbio.bcbioDir + "/final/latest_bcbio"
		for (SequencedSample s : sequencingData.samples) {
			samplePath := sequencingData.analysisDir + "/$s.id" + "/bcbio/final/latest_bcbio"
			programsFile := "$samplePath/programs.txt"
			dataVersionsFile := "$samplePath/data_versions.csv"
			projectSummaryFile := "$samplePath/project-summary.yaml"
			# Copy programs.txt file - all versions are the same
			if (programsFile.exists()) {
				cpCmd := "cp $programsFile $projPath"
				sys $cpCmd
			}
			# Copy data_versions.csv file - all versions are the same
			if (dataVersionsFile.exists()) {
				cpCmd := "cp $dataVersionsFile $projPath"
				sys $cpCmd
			}

			if (projectSummaryFile.exists()) {
				projectSummaryFiles.add(projectSummaryFile)
			}
		}
		# combine project summary files
		combineProjectSummaryYamlFiles( projectSummaryFiles, "proj/Pathproject-summary.yaml" )
	}

	# combines a list of samples project summary files, excluding the redundant lines
	void combineProjectSummaryYamlFiles(string[] samplesProjectSummaryFiles, string outputProjectSummaryFile) {
		# copy the first yaml completely
		firstFile := samplesProjectSummaryFiles.head()
		combinedContent := firstFile.read()

		# concatenate the rest of the files excluding the fist 4 lines (they are redundant and will make bcbiornase fail)
		for (string inputFile : samplesProjectSummaryFiles.tail()) {
			combinedContent += "\n"
			combinedContent += getSampleDescriptionFromProjectSummary(inputFile)
		}

		# bcbiornaseq expects a line jump at the end of the file
		combinedContent += "\n"

		outputProjectSummaryFile.write(combinedContent)
	}

	# Create bcbio final latest dir (it doesn't exist when samples are processed on different instances)
	void createBcbio(bool createSamplesDirectories) {
		logApi("Aggregation: Creating analysis file structure")
		string[] fqsEmpty
		Bcbio bcbio = sequencingData.bcbio
		bcbio.finalLatestDir.mkdir()

		# Create analysis configuration files
		bcbio.createCsv(bcbio.csvFile)
		bcbio.createYaml()

		# Create analysis yaml
		# Note: We create 'fake' input files, otherwise analysis fails
		fqsDemux := sequencingData.demux.fastqs()
		createFakeFastqs(fqsDemux)
		yamlConfig := bcbio.createConfig(fqsDemux, false)
		wait

		if (createSamplesDirectories) {
			# Create dir+links for each sample
			for(SequencedSample s : sequencingData.uniqueSamples()) createBcbio(s)
		}
	}

	# Create analysis final latest dir for sample 'sample'
	# Note: Seq2C files will be added by Seq2c.merge()
	void createBcbio(SequencedSample sample) {
		logApi("Aggregation: Creating bcbio files: Sample '$sample.id' ")

		# Sample dirs: i.e. from individual sample procesing
		sampleFinalLatestDir := "$sequencingData.analysisDir/$sample.id/bcbio/final/latest_bcbio"
		sampleFinalDir := "$sequencingData.analysisDir/$sample.id/bcbio/final/$sample.name"

		# Project dirs: How data is expected when processing all samples toghether
		projectFinalLatestDir := "$sequencingData.analysisDir/bcbio/final/latest_bcbio"
		projectSampleDir := "$sequencingData.analysisDir/bcbio/final/$sample.name"
		projectFinalLatestDir.mkdir()
		projectSampleDir.mkdir()

		# BAM file/s
		ln("$sampleFinalDir/$sample.name-ready.bam", "$projectSampleDir/$sample.name-ready.bam")
		ln("$sampleFinalDir/$sample.name-ready.bam.bai", "$projectSampleDir/$sample.name-ready.bam.bai")

		# Link Vardict file/s
		for (string vf : sampleFinalLatestDir.dir("*-vardict-annotated.vcf.gz*")) {
			ln("$sampleFinalLatestDir/$vf", "$projectFinalLatestDir/$vf")
		}
		sample_batch := sample.getBatch()
#		ln("$sampleFinalLatestDir/$sample_batch-vardict-annotated.vcf.gz", "$projectFinalLatestDir/$sample_batch-vardict-annotated.vcf.gz")

		# Link coverage file/s
		ln("$sampleFinalDir/qc/coverage/$sample.name-coverage.regions.bed.gz", "$projectSampleDir/qc/coverage/$sample.name-coverage.regions.bed.gz")

		# CnvKit file/s
		ln("$sampleFinalDir/$sample_batch-cnvkit-gainloss.txt", "$projectSampleDir/$sample_batch-cnvkit-gainloss.txt")

		# Link SV file/s
		for(string svcaller: ['cnvkit', 'manta', 'seq2c']) {
			for(string ext: ['gz', 'gz.tbi']) {
				ln("$sampleFinalDir/$sample_batch-sv-prioritize-$svcaller\.vcf.$ext", "$projectSampleDir/$sample_batch-sv-prioritize-$svcaller\.vcf.$ext")
			}
		}
		ln("$sampleFinalDir/$sample_batch-sv-prioritize.tsv", "$projectSampleDir/$sample_batch-sv-prioritize.tsv")
	}

	# Create 'fake' fastq files
	# Bcbio yaml checks if fastq files exists
	void createFakeFastqs(string[] fqsDemux) {
		fqs := sequencingData.bcbio.convertPathToAnalysisDir(fqsDemux, false)
		for(string fq: fqs) {
			if( !fq.exists() ) {
				fq.dirName().mkdir()
				fq.write("> fake_fastq")
			}
		}
	}

	# Download data from bucket for local processing
	void download() {
		analysisPath := sequencingData.getAnalysisDir(true)
		# Download each sample
		for(SequencedSample s : sequencingData.uniqueSamples()) {
			logApi("Aggregation: Downloading sample '$s.id'")
			pt := sequencingData.projectType.toLower()
			src := "$analysisPath/$s.id/bcbio/"
			dst := "$sequencingData.analysisDir/$s.id/bcbio/"
			logApi("Aggregation: downloading bcbio dir for sample '$s.id'")
			cloud.upDownLoad.downloadDir(src, dst, excludeBamCram, true)
		}

	}

	string[] getBcbioConfigFiles() {
		bcbio := sequencingData.bcbio
		string[] bcbioConfigFiles = [bcbio.csvFile, bcbio.yamlFile]

		return bcbioConfigFiles
	}

	Ngb getNgb() {
		# we will pass testMode and cloud paramters to Ngb constructor if we are in cloud, or NGB CLI for SCP runs
		if (cloudInstance) {
			testMode := seqautoMode == 'test' || seqautoMode == 'dev' || seqautoMode == "local" ? true : false
			return new Ngb(ngbrunnerDirectory, ngbServerUrl, cloudInstance, testMode)
		} else {
			return new Ngb(ngbrunnerDirectory, ngbCliBin)
		}
	}

	string getNgbRegisterProjectCommand() {
		# generate json file with project samples and files
		Ngb ngb = getNgb()
		command := ngb.registerProjectCommand(sequencingData)
		this.ngbJsonFilePath = ngb.ngbJsonFilePath
		return command
	}

	# get the sample description part of a project summary yaml file, in order to combine several ones into a single file
	string getSampleDescriptionFromProjectSummary(string projectSummaryFile) {
		string[] lines = projectSummaryFile.readLines()

		bool header = true
		string[] outputLines
		for (string line : lines) {
			# the lines before the first '-description' tag are the header ones and should be excluded
			if (line.startsWith('- description:')) header = false
			if (!header) outputLines.add(line)
		}

		return outputLines.join("\n")
	}

	# Create a symlink
	# Note: Ignore if source doesn't exsits (do not fail)
	void ln(string src, string dst) {
		if( !src.exists() ) {
			println "WARNING: Source file '$src' does not exists. Skipping link"
			return
		}
		dst.dirName().mkdir()
		srcCanon := src.pathCanonical()
		println "DEBUG Aggregation.ln: Link '$srcCanon' '$dst'"
		sys ln -s $srcCanon $dst || true
	}

	string[] run() {
		# filter samples by fastq file size: small fastqs, if any, were already excluded from the analysis
		sequencingData.filterByFastqSize()

		if(cloudInstance) {
			download()
			# Create bcbio final dir structure for solveBio uploader, including samples directories
			if( !sequencingData.bcbio.yamlFile.exists() ) createBcbio(true)
		}
		logApi("Aggregation: Start")

		string[] outFiles

		string[] bcbioConfigFiles = getBcbioConfigFiles()

		# Upload BAM files to NGB
		if( doUploadNgb ) uploadToNgb(bcbioConfigFiles)

		# seq2c merge
		seq2cAll := seq2c.merge(bcbioConfigFiles)
		# seq2cAll can be empty if sequencing data is WGS or seq2c merge disabled via config
		if ( !seq2cAll.isEmpty() ) outFiles += seq2cAll

		# These steps have to be executed after Seq2c
		in := bcbioConfigFiles + outFiles

		# MultiQc agregate report: Only if this was run as a cloud instance
		if( cloudInstance ) outFiles += multiQc.run(in)

		# Combine and upload per-sample metadata files
		if (cloudInstance) {
			combineConfigurationFiles()
		}

		# TODO: we can replace this wait with file dependencies to build a dependency graph (i.e., task 'B' depends on the output files of task 'A')
		wait

		# SolveBio upload
		if( doUploadSolveBio ) outFiles += solveBio.upload(in)

		wait
		uploadS3()
		logApi("Aggregation: End")
		return outFiles
	}

	# Upload files to S3
	void uploadS3() {
		UpDownLoad upload = sequencingData.getUpDownLoad()
		cloudProjectPath := sequencingData.getAnalysisDir(true)

		localLatestBcbioPath := sequencingData.bcbio.bcbioDir + "/final/latest_bcbio"
		s3LatestBcbioPath := "$cloudProjectPath/bcbio/final/latest_bcbio"
		upload.uploadDir(localLatestBcbioPath, s3LatestBcbioPath)
		# Upload multiqc dir
		# Upload seq2c files

		# upload bcbio config directory
		localBcbioConfigPath := sequencingData.bcbio.bcbioDir + "/config"
		s3BcbioConfigPath := "$cloudProjectPath/bcbio/config"
		upload.uploadDir(localBcbioConfigPath, s3BcbioConfigPath)

		# upload NGB config file if ngb was executed
		if (this.ngbJsonFilePath != '') {
			upload.upload(this.ngbJsonFilePath, "$cloudProjectPath/ngb.json")
		}
	}

	# Upload files to NGB (browser)
	string uploadToNgb(string[] bcbioConfigFiles) {
		bcbio := sequencingData.bcbio

		string[] inFiles = bcbioConfigFiles

		output := "$bcbio.finalLatestDir/ngb_upload.done"

		logApi("Aggregation: Upload to NGB")
		log("\tinputs  :\n\t\t" + inFiles.join("\n\t\t"))
		log("\toutput  : $output")
		lcmd := logApiCmd('INFO', 'Upload to NGB: Upload to NGB. Finished')
		ngbRegisterCommand := getNgbRegisterProjectCommand()
		task(output <- inFiles, system := 'local', taskName := "uploadToNgb_$taskId", retry := 3) {
			sys $modules
			sys cd $bcbio.analysisDir/bcbio
			sys $ngbRegisterCommand
			sys date > $output
			sys $lcmd
		}

		return output
	}
}

#-------------------------------------------------------------------------------
# Aggregate results: Single Cell (10x)
#-------------------------------------------------------------------------------
class Aggregation10x extends Aggregation {
	string csvFile
	string analysisPath
	string[] aggrRuns
	SolveBio solveBio
	string[] solveBioFolders
	string solveBioTags
	string solveBioExclude
	SequencingData sequencingData

	# Constructor
	void Aggregation10x(SequencingData sequencingData) {
		super.Aggregation(sequencingData)
		this.sequencingData = sequencingData
		aggrRuns = [ "not_normalized,none", "normalized,mapped" ]
		solveBioTags = "AZ-NGS,$genomeVersion,$projectType,$sequencingType"
		solveBioExclude = "analysis"
	}

	# Create CSV file for aggregation
	void createCsvFile(string sampleSheetFile) {
		_sampleSheet := new CellrangerAggrSampleSheet(sequencingData.uniqueSamples())
		sampleSheetFile.write(_sampleSheet.toCsv())
	}

	# Download data from bucket for local processing
	void download() {
		# Download each sample
		for(SequencedSample s : sequencingData.uniqueSamples()) {
			src := "$analysisPath/$s.name"
			dst := "$sequencingData.analysisDir/$s.name"
			logApi("Aggregation: Downloading 10x dir for sample '$s.name'")
			cloud.upDownLoad.downloadDir(src, dst)
		}
		src := "$analysisPath/metadata"
		dst := "$sequencingData.analysisDir/metadata"
		logApi("Aggregation: Downloading 10x metadata dir")
		cloud.upDownLoad.downloadDir(src, dst)

	}

	# initialize fields that depend on sequencingData object
	void init(SequencingData sequencingData) {
		csvFile = "$sequencingData.analysisDir/$cellrangerAggrCsvFile"
		analysisPath = sequencingData.getAnalysisDir(true)
		solveBio = new SolveBio(solveBioSingleCellVault, sequencingData.project)
	}

	void run() {
		init(sequencingData)

		if (cloudInstance) {
			download()
		}
		createCsvFile(this.csvFile)
		for (string aggrs : aggrRuns) {
			idNorm := aggrs.split(",")
			runAggr(idNorm[0], idNorm[1])
		}
		
		upload()
		solveBioUpload()
	}

	void runAggr(string id, string normalization) {
		aggrCmd := "cellranger aggr --id=$id --csv=$csvFile --normalize=$normalization"
		outputFiles := [ "$sequencingData.analysisDir/$id/outs/count/filtered_feature_bc_matrix.h5" ]
		inputFiles := [ csvFile ]
		task(outputFiles <- inputFiles, mem := cellrangerMem, taskName := "Cellranger_aggr_$taskId", timeout := max(bcbioTimeoutIpython, bcbioTimeoutTask)) {
			sys cd $sequencingData.analysisDir
			sys $aggrCmd
		}

		wait
	}

	string[] solveBioUpload() {
		return solveBioUpload(false)
	}

	string[] solveBioUpload(bool dryrun) {
		if (!dryrun) {
			renameCommand := "ln -s $sequencingData.analysisDir/normalized/outs $sequencingData.analysisDir/normalized_outs"
			sys $renameCommand || true
			renameCommand = "ln -s $sequencingData.analysisDir/not_normalized/outs $sequencingData.analysisDir/not_normalized_outs"
			sys $renameCommand || true
		}
		uploadFolders := "$sequencingData.analysisDir/metadata"
		uploadFolders += " $sequencingData.analysisDir/normalized_outs"
		uploadFolders += " $sequencingData.analysisDir/not_normalized_outs"

		solveBioCommands := solveBio.getUploadCommands(uploadFolders,  solveBioTags, solveBioExclude)

		if (dryrun) {
			return solveBioCommands
		}
		
		string[] results
		for (string command : solveBioCommands) {
			res := sys $command || true
			results.add(res)
		}
		return results
	}

	string[] solveBioUploadTest() {
		return solveBioUpload(true)
	}

	void upload() {
		for (string aggrs : aggrRuns) {
			idNorm := aggrs.split(",")
			src := "$sequencingData.analysisDir/$idNorm[0]/outs"
			dst := "$analysisPath/$idNorm[0]/"
			cloud.upDownLoad.uploadDir(src, dst)
		}
		cloud.upDownLoad.upload(csvFile, analysisPath)
	}
}

#-------------------------------------------------------------------------------
# Aggregate results: RnaSeq
#-------------------------------------------------------------------------------
class RNASeqAggregation extends Aggregation {
  OnRamp onRamp

	void RNASeqAggregation(SequencingData sd) {
	 	sequencingData = sd
		solveBio = new SolveBio(sd, solveBioUploadImport, solveBioVault)
		multiQc = new MultiQc(sequencingData)
	}

	bool shouldUploadToOnRamp() {
		# check if sequencing type and genome are allowed for OnRamp
		if (doUploadOnRamp && (sequencingData.sequencingType == 'RNASeq' || sequencingData.sequencingType == 'miRNASeq')) {
			if(["hg19", "hg38", "mm10"].has(sequencingData.genomeVersion)) {
				return true
			} else {
				logApi("The genome $sequencingData.genomeVersion is not supported by OnRamp. Skipping onRamp upload")
			}
		}

		return false
	}

	# Compress the rnaseq result files to be archived in S3
	void gzRnaseqResults(string[] rnaseqResultFiles) {
		for (string rnaSeqResultFile : rnaseqResultFiles) {
			# remove ".gz" from file name
			uncompressedFileName := rnaSeqResultFile.removeExt()
			sys gzip $uncompressedFileName
		}
	}

	string[] run() {
		fatalerror("This is an abstract class: use RNASeqCloudAggregation or RNASeqSCPAggregation instead")
		return []
	}

	string[] solveBioUpload(string[] bcbioConfigFiles, string[] rnaseqResultFiles) {

		# decompress bcbio rnaseq result files and do solvebio upload
		unzipRnaseqResults(rnaseqResultFiles)

		solveBioInputFiles := bcbioConfigFiles + rnaseqResultFiles
		outFiles := [solveBio.upload(solveBioInputFiles)]
		wait

		# compress bcbio rna seq result files
		gzRnaseqResults(rnaseqResultFiles)

		return outFiles
	}

	# Uncompress the rnaseq result files to be uploaded to Solvebio
	void unzipRnaseqResults(string[] rnaseqResultFiles) {
		for (string rnaSeqResultFile : rnaseqResultFiles) {
			sys gzip -d $rnaSeqResultFile
		}
	}

	# Upload files to S3
	void uploadS3() {
		super.uploadS3()

		UpDownLoad upload = sequencingData.getUpDownLoad()
		cloudProjectPath := sequencingData.getAnalysisDir(true)

		# upload also bcbioRNASeq directory (unless skipped)
		localBcbiornaseqPath := sequencingData.bcbio.bcbioDir + "/final/bcbioRNASeq"
		s3BcbioRnaseqPath := sequencingData.getAnalysisDir(true) + "/bcbio/final/bcbioRNASeq"
		upload.uploadDir(localBcbiornaseqPath, s3BcbioRnaseqPath)

		# upload NGB config file if ngb was executed
		if (this.ngbJsonFilePath != '') {
			upload.upload(this.ngbJsonFilePath, "$cloudProjectPath/ngb.json")
		}
	}
}


#-------------------------------------------------------------------------------
# Aggregate results: RnaSeq (Cloud processing)
#-------------------------------------------------------------------------------
class RNASeqCloudAggregation extends RNASeqAggregation {
	void RNASeqCloudAggregation(SequencingData sd) {
	 	sequencingData = sd
		solveBio = new SolveBio(sd, solveBioUploadImport, solveBioVault)
		multiQc = new MultiQc(sequencingData)
		onRamp = new OnRamp(sd, onrampScriptPath, seqautoMode)
	}

	string[] bcbiornaseq(string[] inputFiles) {
		bcbio := sequencingData.bcbio
		date := sys date +%Y-%m-%d
		date = date.split('\n')[0]

		# make 'dated' directory symlink, because bcbiornaseq does not recognises 'latest_bcbio'
		string symlinkCommand = "ln -s $bcbio.finalLatestDir $bcbio.finalDir/$date" + "_bcbio"
		sys $symlinkCommand || true

		# link bcbio final directory for each individual sample in the common one
		for(SequencedSample s : sequencingData.uniqueSamples()) {
			sys ln -s $sequencingData.analysisDir/$s.id/bcbio/final/$s.name $sequencingData.analysisDir/bcbio/final/$s.name
		}

		qualityControlDir := "$bcbio.finalDir/bcbioRNASeq/results/$date/quality_control"
		rnaseqResultFiles := ["$qualityControlDir/normalized_counts.csv.gz", "$qualityControlDir/raw_counts.csv.gz", "$qualityControlDir/tpm.csv.gz"]

		logApi("RNASeqCloudAggregation: Executing bcbiornaseq")
		lcmd := logApiCmd('INFO', 'RNASeqCloudAggregation: bcbiornaseq finished')

		bcioRnaSeqCommand := getBcbioRnaSeqCommandLine(bcbio.finalDir, rnaSeqInterestingGroups)
		bcbioWorkDir := "$bcbio.bcbioDir/work"
		task(rnaseqResultFiles <- inputFiles, system := 'local', taskName := "bcbiornaseq_$taskId") {
			sys cd $bcbioWorkDir
			sys $bcioRnaSeqCommand
			sys $lcmd
		}

		return rnaseqResultFiles
	}

	string[] copyLatestBcbioCommonFiles(string inputDirectory, string outputDirectory) {
		commonFiles := ['tx2gene.csv', 'metadata.csv', 'programs.txt', 'data_versions.csv', 'bcbio-nextgen.log', 'bcbio-nextgen-commands.log']

		string[] outputFiles
		for (string commonFile : commonFiles) {
			outputPath := "$outputDirectory/$commonFile"
			sys cp $inputDirectory/$commonFile $outputPath
			outputFiles.add(outputPath)
		}

		return outputFiles
	}

	# creates a project level bcbio/final/latest_bcbio directory with the files needed to run bcbiornaseq
	string[] createLatestBcbio(string analysisDir, SequencedSample[] samples) {
		Bcbio bcbio = sequencingData.bcbio
		projectFinalLatestDir := "$bcbio.finalLatestDir"
		projectFinalLatestDir.mkdir()

		# Copy the common 'latest_bcbio' files from one sample 'latest_bcbio' directory to the project common one
		# We copy only one because those files are identical for every sample
		sampleId := samples.head().id
		samplePath := sequencingData.analysisDir + "/$sampleId/bcbio/final/latest_bcbio"
		string[] bcbioLatestFiles = copyLatestBcbioCommonFiles(samplePath, projectFinalLatestDir)

		string[] samplesProjectSummaryFiles
		for (SequencedSample s : samples) {
			projectSummaryFile := sequencingData.analysisDir + "/$s.id/bcbio/final/latest_bcbio/project-summary.yaml"
			samplesProjectSummaryFiles.add(projectSummaryFile)
		}
		combinedProjectSummaryPath := "$projectFinalLatestDir/project-summary.yaml"
		combineProjectSummaryYamlFiles(samplesProjectSummaryFiles, combinedProjectSummaryPath)

		bcbioLatestFiles.add(combinedProjectSummaryPath)

		return bcbioLatestFiles
	}

	# Download bcbio final directory for each sample from bucket for local processing
	void download() {
		analysisPath := sequencingData.getAnalysisDir(true)
		# Download each sample
		for(SequencedSample s : sequencingData.uniqueSamples()) {
			logApi("Aggregation: Downloading sample '$s.id'")
			pt := sequencingData.projectType.toLower()
			src := "$analysisPath/$s.id/bcbio/final"
			dst := "$sequencingData.analysisDir/$s.id/bcbio/final"
			logApi("Aggregation: downloading bcbio/final dir for sample '$s.id'")
			cloud.upDownLoad.downloadDir(src, dst, excludeBamCram + excludeFastas, true)
		}
	}

	string getBcbioRnaSeqCommandLine(string bcbioFinalDir, string interestingGroups) {
		groups := interestingGroups.isEmpty() ? '' : ' --interestingGroups "' + interestingGroups + '"'
		organism := sequencingData.genomeVersion == "mm10" ? "mus musculus" : "homo sapiens"
		return seqautoHomeDir + '/scripts/bcbioRnaSeq/bcbiornaseq.sh --uploadDir ' + bcbioFinalDir + ' --organism "' + organism + '"' + groups
	}

	string[] run() {
		# filter samples by fastq file size: small fastqs, if any, were already excluded from the analysis
		sequencingData.filterByFastqSize()

		if (!cloudInstance) {
			fatalerror("RNASeq aggregation only works on cloud")
		}

		# download 'bcbio' directory for each sample
		download()

		# Create bcbio final dir structure for solveBio uploader, excluding samples directories
		createBcbio(false)

		logApi("RNASeqCloudAggregation: Start")

		string[] bcbioConfigFiles = getBcbioConfigFiles()

		# Upload BAM files to NGB
		if( doUploadNgb ) uploadToNgb(bcbioConfigFiles)

		# create a common latest bcbio directory, with the files needed by bcbiornaseq
		bcbioLatestFiles := createLatestBcbio(sequencingData.analysisDir, sequencingData.uniqueSamples())

		# execute bcbiornaseq
		bcbioRnaSeqFiles := bcbiornaseq(bcbioLatestFiles)
		wait

		# MultiQc agregate report
		string[] inFiles = bcbioConfigFiles + bcbioLatestFiles
		outFiles := multiQc.run(inFiles)

		wait
		# Upload files to OnRamp
		if( shouldUploadToOnRamp() ) onRamp.upload(inFiles)

		wait
		# SolveBio upload
		if( doUploadSolveBio ) {
			solveBioUpload(inFiles, bcbioRnaSeqFiles)
		}

		uploadS3()
		logApi("RNASeqCloudAggregation: End")
		return outFiles
	}
}


#-------------------------------------------------------------------------------
# ATACSeqCloudAggregation Aggregation
#-------------------------------------------------------------------------------
class ATACSeqCloudAggregation extends Aggregation {
	Ataqv ataqv
	ConsensusPeaks consensus

	void ATACSeqCloudAggregation(SequencingData sd) {
		sequencingData = sd
		multiQc = new MultiQc(sequencingData)
		ataqv = new Ataqv(sequencingData)
		consensus = new ConsensusPeaks(sequencingData)
		solveBio = new SolveBio(sequencingData, solveBioUploadImport, solveBioVault, 'ATACSEQ')
	}

	# Download bcbio final directory for each sample from bucket for local processing
	void download() {
		analysisPath := sequencingData.getAnalysisDir(true)
		# Download each sample
		for(SequencedSample s : sequencingData.uniqueSamples()) {
			logApi("ATACSeqCloudAggregation: Downloading sample '$s.id'")
			pt := sequencingData.projectType.toLower()
			src := "$analysisPath/$s.id/bcbio/final"
			dst := "$sequencingData.analysisDir/$s.id/bcbio/final"
			logApi("ATACSeqCloudAggregation: downloading bcbio/final dir for sample '$s.id'")
			cloud.upDownLoad.downloadDir(src, dst, excludeBamCram + excludeFastas + excludeEpi, true)
		}
	}

	# creates a project level bcbio/final/latest_bcbio directory
	string[] createLatestBcbio(string analysisDir, SequencedSample[] samples) {
		Bcbio bcbio = sequencingData.bcbio
		projectFinalLatestDir := "$bcbio.finalLatestDir"
		projectFinalLatestDir.mkdir()

		# Copy the common 'latest_bcbio' files from one sample 'latest_bcbio' directory to the project common one
		# We copy only one because those files are identical for every sample
		sampleId := samples.head().id
		samplePath := sequencingData.analysisDir + "/$sampleId/bcbio/final/latest_bcbio"
		commonFiles := ['project-summary.yaml', 'metadata.csv', 'programs.txt', 'data_versions.csv', 'bcbio-nextgen.log', 'bcbio-nextgen-commands.log']
		string[] bcbioLatestFiles
		for (string commonFile : commonFiles) {
			outputPath := "$projectFinalLatestDir/$commonFile"
			sys cp $samplePath/$commonFile $outputPath
			bcbioLatestFiles.add(outputPath)
		}

		return bcbioLatestFiles
	}

	# Upload files to S3
	void uploadS3() {
		super.uploadS3()

		UpDownLoad upload = sequencingData.getUpDownLoad()
		cloudProjectPath := sequencingData.getAnalysisDir(true)

		# upload NGB config file if ngb was executed
		if (this.ngbJsonFilePath != '') {
			upload.upload(this.ngbJsonFilePath, "$cloudProjectPath/ngb.json")
		}
	}

	string[] run() {
		if (!cloudInstance) {
			fatalerror("ATACSeq aggregation only works on cloud")
		}

		logApi("ATACSeqCloudAggregation: Start")

		# download 'bcbio' directory for each sample
		download()

		# Create bcbio final dir structure for solveBio uploader, excluding samples directories
		createBcbio(false)

		string[] bcbioConfigFiles = getBcbioConfigFiles()

		# Upload BAM files to NGB
		if( doUploadNgb ) uploadToNgb(bcbioConfigFiles)

		# create a common latest bcbio directory
		bcbioLatestFiles := createLatestBcbio(sequencingData.analysisDir, sequencingData.uniqueSamples())

		# MultiQc aggregate report
		string[] inFiles = bcbioConfigFiles + bcbioLatestFiles
		outFiles := multiQc.run(inFiles)

		# Ataqv aggregate report
		outFiles += ataqv.run()

		# ConsensusPeaks aggregate report
		consensusFiles := consensus.run()
		outFiles += consensusFiles

		wait

		# SolveBio upload
		solveBioFiles := bcbioConfigFiles + consensusFiles
		if( doUploadSolveBio ) {
			solveBio.upload(solveBioFiles)
			wait
		}

		uploadS3()
		logApi("ATACSeqCloudAggregation: End")
		return outFiles
	}
}
